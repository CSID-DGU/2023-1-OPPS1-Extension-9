{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tc4UHZR_DQE"
      },
      "source": [
        "**bert**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "14BNWcMtIXZi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rF62eXBIaYm",
        "outputId": "f5e4b8cd-029f-4fe9-c03d-cc9420c2e2db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: boto3==1.15.18 in /usr/local/lib/python3.10/dist-packages (1.15.18)\n",
            "Requirement already satisfied: botocore<1.19.0,>=1.18.18 in /usr/local/lib/python3.10/dist-packages (from boto3==1.15.18) (1.18.18)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3==1.15.18) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from boto3==1.15.18) (0.3.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.19.0,>=1.18.18->boto3==1.15.18) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.26,>=1.20 in /usr/local/lib/python3.10/dist-packages (from botocore<1.19.0,>=1.18.18->boto3==1.15.18) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.0,>=1.18.18->boto3==1.15.18) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install boto3==1.15.18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hejrBHKcH9Ta",
        "outputId": "a6afbe99-db2a-422a-8fd5-28d7e44fa9dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gluonnlp==0.8.0 in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.8.0) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "pip install gluonnlp==0.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6qNE1VylS-q",
        "outputId": "170501f0-95e4-46fc-f1eb-909ba42f66c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mxnet==1.5.0 in /usr/local/lib/python3.10/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet==1.5.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet==1.5.0) (2.27.1)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet==1.5.0) (0.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.5.0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.5.0) (3.4)\n"
          ]
        }
      ],
      "source": [
        "pip install mxnet==1.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihzX5Ta_IPA_",
        "outputId": "1507c599-107a-44c0-c59f-489fab912c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4KOUS9lIJ4W",
        "outputId": "3ed04fde-4e20-46b4-ebdf-2f23816ea885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.15.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.3.3)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.11.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYEGp4YblmZ7",
        "outputId": "19a9e5bd-6c92-4ba0-ff05-96fd85712404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4jBS6gylqDg",
        "outputId": "83e8fda4-ae01-4e42-d177-3f661b08e4f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece==0.1.96\n",
            "  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.99\n",
            "    Uninstalling sentencepiece-0.1.99:\n",
            "      Successfully uninstalled sentencepiece-0.1.99\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "pip install sentencepiece==0.1.96"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jCBmEzL9_D4_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT, self).__init__()\n",
        "        self.bert = bert\n",
        "\n",
        "    def forward(self, inputs, mode):\n",
        "\n",
        "        if mode == 'train':\n",
        "            anchor_attention_mask = self.gen_attention_mask(inputs['anchor']['source'],\n",
        "                                                            inputs['anchor']['valid_length'])\n",
        "\n",
        "            positive_attention_mask = self.gen_attention_mask(inputs['positive']['source'],\n",
        "                                                              inputs['positive']['valid_length'])\n",
        "\n",
        "            negative_attention_mask = self.gen_attention_mask(inputs['negative']['source'],\n",
        "                                                              inputs['negative']['valid_length'])\n",
        "\n",
        "            _, anchor_pooler = self.bert(input_ids=inputs['anchor']['source'],\n",
        "                                         token_type_ids=inputs['anchor']['segment_ids'],\n",
        "                                         attention_mask=anchor_attention_mask)\n",
        "\n",
        "            _, positive_pooler = self.bert(input_ids=inputs['positive']['source'],\n",
        "                                           token_type_ids=inputs['positive']['segment_ids'],\n",
        "                                           attention_mask=positive_attention_mask)\n",
        "\n",
        "            _, negative_pooler = self.bert(input_ids=inputs['negative']['source'],\n",
        "                                           token_type_ids=inputs['negative']['segment_ids'],\n",
        "                                           attention_mask=negative_attention_mask)\n",
        "\n",
        "            return anchor_pooler, positive_pooler, negative_pooler\n",
        "\n",
        "        else:\n",
        "            sentence_1_attention_mask = self.gen_attention_mask(inputs['sentence_1']['source'],\n",
        "                                                                inputs['sentence_1']['valid_length'])\n",
        "\n",
        "            sentence_2_attention_mask = self.gen_attention_mask(inputs['sentence_2']['source'],\n",
        "                                                                inputs['sentence_2']['valid_length'])\n",
        "\n",
        "            _, sentence_1_pooler = self.bert(input_ids=inputs['sentence_1']['source'],\n",
        "                                             token_type_ids=inputs['sentence_1']['segment_ids'],\n",
        "                                             attention_mask=sentence_1_attention_mask)\n",
        "\n",
        "            _, sentence_2_pooler = self.bert(input_ids=inputs['sentence_2']['source'],\n",
        "                                             token_type_ids=inputs['sentence_2']['segment_ids'],\n",
        "                                             attention_mask=sentence_2_attention_mask)\n",
        "\n",
        "            return sentence_1_pooler, sentence_2_pooler\n",
        "\n",
        "    def encode(self, inputs, device):\n",
        "\n",
        "        attention_mask = self.gen_attention_mask(inputs['source'], inputs['valid_length'])\n",
        "\n",
        "        _, embeddings = self.bert(input_ids=inputs['source'].to(device),\n",
        "                                  token_type_ids=inputs['segment_ids'].to(device),\n",
        "                                  attention_mask=attention_mask.to(device))\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "\n",
        "        return attention_mask.float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG52gmcY_L-H"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTTHeOcj-7DX"
      },
      "source": [
        "**utils**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUHsJ0CX_z5E",
        "outputId": "680a2b9f-fdd3-49ab-fc64-aabfffa5b9c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.10/dist-packages (2.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (23.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (3.20.3)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorboardx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ipL8Db9G-7qt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import logging\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "writer = SummaryWriter()\n",
        "\n",
        "\n",
        "class Metric():\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "\n",
        "    def get_lr(self, optimizer):\n",
        "        return optimizer.state_dict()['param_groups'][0]['lr']\n",
        "\n",
        "    def count_parameters(self, model):\n",
        "        print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "    def cal_acc(self, yhat, y):\n",
        "        with torch.no_grad():\n",
        "            yhat = yhat.max(dim=-1)[1]  # [0]: max value, [1]: index of max value\n",
        "            acc = (yhat == y).float().mean()\n",
        "\n",
        "        return acc\n",
        "\n",
        "    def cal_time(self, start_time, end_time):\n",
        "        elapsed_time = end_time - start_time\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "\n",
        "        return elapsed_mins, elapsed_secs\n",
        "\n",
        "    def cal_dev_score(self, score, indicator):\n",
        "        validation_score = score['score'] / score['iter']\n",
        "        for key, value in indicator.items():\n",
        "            indicator[key] /= score['iter']\n",
        "\n",
        "        print(\"\\n\\nCosine-Similarity :\\tPearson: {:.4f}\\tSpearman: {:.4f}\".format(\n",
        "            indicator['eval_pearson_cosine'], indicator['eval_spearman_cosine']))\n",
        "        print(\"Manhattan-Distance:\\tPearson: {:.4f}\\tSpearman: {:.4f}\".format(\n",
        "            indicator['eval_pearson_manhattan'], indicator['eval_spearman_manhattan']))\n",
        "        print(\"Euclidean-Distance:\\tPearson: {:.4f}\\tSpearman: {:.4f}\".format(\n",
        "            indicator['eval_pearson_euclidean'], indicator['eval_spearman_euclidean']))\n",
        "        print(\"Dot-Product-Similarity:\\tPearson: {:.4f}\\tSpearman: {:.4f}\\n\".format(\n",
        "            indicator['eval_pearson_dot'], indicator['eval_spearman_dot']))\n",
        "\n",
        "        return validation_score\n",
        "\n",
        "    def update_indicator(self, indicator, score):\n",
        "        for key, value in indicator.items():\n",
        "            if key == 'eval_spearman_cosine':\n",
        "                indicator[key] += score['eval_spearman_cosine']\n",
        "            elif key == 'eval_pearson_cosine':\n",
        "                indicator[key] += score['eval_pearson_cosine']\n",
        "            elif key == 'eval_spearman_manhattan':\n",
        "                indicator[key] += score['eval_spearman_manhattan']\n",
        "            elif key == 'eval_pearson_manhattan':\n",
        "                indicator[key] += score['eval_pearson_manhattan']\n",
        "            elif key == 'eval_spearman_euclidean':\n",
        "                indicator[key] += score['eval_spearman_euclidean']\n",
        "            elif key == 'eval_pearson_euclidean':\n",
        "                indicator[key] += score['eval_pearson_euclidean']\n",
        "            elif key == 'eval_spearman_dot':\n",
        "                indicator[key] += score['eval_spearman_dot']\n",
        "            elif key == 'eval_pearson_dot':\n",
        "                indicator[key] += score['eval_pearson_dot']\n",
        "\n",
        "    def draw_graph(self, cp):\n",
        "        writer.add_scalars('loss_graph', {'train': cp['tl'], 'valid': cp['vl']}, cp['ep'])\n",
        "        writer.add_scalars('acc_graph', {'train': cp['tma'], 'valid': cp['vma']}, cp['ep'])\n",
        "\n",
        "    def performance_check(self, cp, config):\n",
        "        print(f'\\t==Epoch: {cp[\"ep\"] + 1:02} | Epoch Time: {cp[\"epm\"]}m {cp[\"eps\"]}s==')\n",
        "        print(f'\\t==Train Loss: {cp[\"tl\"]:.4f} | Train acc: {cp[\"tma\"]:.4f}==')\n",
        "        print(f'\\t==Valid Loss: {cp[\"vl\"]:.4f} | Valid acc: {cp[\"vma\"]:.4f}==')\n",
        "        print(f'\\t==Epoch latest LR: {self.get_lr(config[\"optimizer\"]):.9f}==\\n')\n",
        "\n",
        "    def print_size_of_model(self, model):\n",
        "        torch.save(model.state_dict(), \"temp.p\")\n",
        "        print('Size (MB):', os.path.getsize(\"temp.p\") / 1e6)\n",
        "        os.remove('temp.p')\n",
        "\n",
        "    def move2device(self, sample, device):\n",
        "        if len(sample) == 0:\n",
        "            return {}\n",
        "\n",
        "        def _move_to_device(maybe_tensor, device):\n",
        "            if torch.is_tensor(maybe_tensor):\n",
        "                return maybe_tensor.to(device)\n",
        "            elif isinstance(maybe_tensor, dict):\n",
        "                return {\n",
        "                    key: _move_to_device(value, device)\n",
        "                    for key, value in maybe_tensor.items()\n",
        "                    }\n",
        "            elif isinstance(maybe_tensor, list):\n",
        "                return [_move_to_device(x, device) for x in maybe_tensor]\n",
        "            elif isinstance(maybe_tensor, tuple):\n",
        "                return [_move_to_device(x, device) for x in maybe_tensor]\n",
        "            else:\n",
        "                return maybe_tensor\n",
        "\n",
        "        return _move_to_device(sample, device)\n",
        "\n",
        "    def save_model(self, config, cp, pco):\n",
        "        if not os.path.exists(config['args'].path_to_save):\n",
        "            os.makedirs(config['args'].path_to_save)\n",
        "\n",
        "        sorted_path = config['args'].path_to_save + config['args'].ckpt\n",
        "        if cp['vs'] > pco['best_valid_score']:\n",
        "            # pco['early_stop_patient'] = 0\n",
        "            pco['best_valid_score'] = cp['vs']\n",
        "\n",
        "            state = {'model': config['model'].state_dict(),\n",
        "                     'optimizer': config['optimizer'].state_dict()}\n",
        "\n",
        "            torch.save(state, sorted_path)\n",
        "            print(f'\\t## SAVE {sorted_path} |'\n",
        "                  f' valid_score: {cp[\"vs\"]:.4f} |'\n",
        "                  f' epochs: {cp[\"ep\"]} |'\n",
        "                  f' steps: {cp[\"step\"]} ##\\n')\n",
        "\n",
        "        # self.draw_graph(cp)\n",
        "        # self.performance_check(cp, config)\n",
        "\n",
        "\n",
        "def pytorch_cos_sim(a, b):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
        "    This function can be used as a faster replacement for 1-scipy.spatial.distance.cdist(a,b)\n",
        "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
        "    \"\"\"\n",
        "    if not isinstance(a, torch.Tensor):\n",
        "        a = torch.tensor(a)\n",
        "\n",
        "    if not isinstance(b, torch.Tensor):\n",
        "        b = torch.tensor(b)\n",
        "\n",
        "    if len(a.shape) == 1:\n",
        "        a = a.unsqueeze(0)\n",
        "\n",
        "    if len(b.shape) == 1:\n",
        "        b = b.unsqueeze(0)\n",
        "\n",
        "    a_norm = a / a.norm(dim=1)[:, None]\n",
        "    b_norm = b / b.norm(dim=1)[:, None]\n",
        "    return torch.mm(a_norm, b_norm.transpose(0, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Utb0jd-4qh"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wOovh__-u1J"
      },
      "source": [
        "**loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IuSuSZW8-uG9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class Loss():\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.cos = nn.CosineSimilarity(dim=-1)\n",
        "        self.metric = Metric(args)\n",
        "\n",
        "    def train_loss_fct(self, config, a, p, n):\n",
        "\n",
        "        positive_similarity = self.cos(a.unsqueeze(1), p.unsqueeze(0)) / self.args.temperature\n",
        "        negative_similarity = self.cos(a.unsqueeze(1), n.unsqueeze(0)) / self.args.temperature\n",
        "        cosine_similarity = torch.cat([positive_similarity, negative_similarity], dim=1).to(self.args.device)\n",
        "\n",
        "        labels = torch.arange(cosine_similarity.size(0)).long().to(self.args.device)\n",
        "\n",
        "        loss = config['criterion'](cosine_similarity, labels)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def evaluation_during_training(self, embeddings1, embeddings2, labels, indicator):\n",
        "\n",
        "        embeddings1 = embeddings1.cpu().numpy()\n",
        "        embeddings2 = embeddings2.cpu().numpy()\n",
        "        labels = labels.cpu().numpy().flatten()\n",
        "\n",
        "        cosine_scores = 1 - (paired_cosine_distances(embeddings1, embeddings2))\n",
        "        manhattan_distances = -paired_manhattan_distances(embeddings1, embeddings2)\n",
        "        euclidean_distances = -paired_euclidean_distances(embeddings1, embeddings2)\n",
        "        dot_products = [np.dot(emb1, emb2) for emb1, emb2 in zip(embeddings1, embeddings2)]\n",
        "\n",
        "        eval_pearson_cosine, _ = pearsonr(labels, cosine_scores)\n",
        "        eval_spearman_cosine, _ = spearmanr(labels, cosine_scores)\n",
        "\n",
        "        eval_pearson_manhattan, _ = pearsonr(labels, manhattan_distances)\n",
        "        eval_spearman_manhattan, _ = spearmanr(labels, manhattan_distances)\n",
        "\n",
        "        eval_pearson_euclidean, _ = pearsonr(labels, euclidean_distances)\n",
        "        eval_spearman_euclidean, _ = spearmanr(labels, euclidean_distances)\n",
        "\n",
        "        eval_pearson_dot, _ = pearsonr(labels, dot_products)\n",
        "        eval_spearman_dot, _ = spearmanr(labels, dot_products)\n",
        "\n",
        "        score = {'eval_pearson_cosine': eval_pearson_cosine,\n",
        "                 'eval_spearman_cosine': eval_spearman_cosine,\n",
        "                 'eval_pearson_manhattan': eval_pearson_manhattan,\n",
        "                 'eval_spearman_manhattan': eval_spearman_manhattan,\n",
        "                 'eval_pearson_euclidean': eval_pearson_euclidean,\n",
        "                 'eval_spearman_euclidean': eval_spearman_euclidean,\n",
        "                 'eval_pearson_dot': eval_pearson_dot,\n",
        "                 'eval_spearman_dot': eval_spearman_dot}\n",
        "\n",
        "        self.metric.update_indicator(indicator, score)\n",
        "\n",
        "        return max(eval_spearman_cosine, eval_spearman_manhattan, eval_spearman_euclidean, eval_spearman_dot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5JkT0dc-r-n"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwJObmIFByp-"
      },
      "source": [
        "**aws_s3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7zyM81-B47h",
        "outputId": "ded7d233-897a-414f-b7e0-78836ada57a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n"
          ]
        }
      ],
      "source": [
        "import boto3\n",
        "import os\n",
        "import sys\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "\n",
        "\n",
        "class AwsS3Downloader(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        aws_access_key_id=None,\n",
        "        aws_secret_access_key=None,\n",
        "    ):\n",
        "        self.resource = boto3.Session(\n",
        "            aws_access_key_id=aws_access_key_id,\n",
        "            aws_secret_access_key=aws_secret_access_key,\n",
        "        ).resource(\"s3\")\n",
        "        self.client = boto3.client(\n",
        "            \"s3\",\n",
        "            aws_access_key_id=aws_access_key_id,\n",
        "            aws_secret_access_key=aws_secret_access_key,\n",
        "            config=Config(signature_version=UNSIGNED),\n",
        "        )\n",
        "\n",
        "    def __split_url(self, url: str):\n",
        "        if url.startswith(\"s3://\"):\n",
        "            url = url.replace(\"s3://\", \"\")\n",
        "        bucket, key = url.split(\"/\", maxsplit=1)\n",
        "        return bucket, key\n",
        "\n",
        "    def download(self, url: str, local_dir: str):\n",
        "        bucket, key = self.__split_url(url)\n",
        "        filename = os.path.basename(key)\n",
        "        file_path = os.path.join(local_dir, filename)\n",
        "\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "        meta_data = self.client.head_object(Bucket=bucket, Key=key)\n",
        "        total_length = int(meta_data.get(\"ContentLength\", 0))\n",
        "\n",
        "        downloaded = 0\n",
        "\n",
        "        def progress(chunk):\n",
        "            nonlocal downloaded\n",
        "            downloaded += chunk\n",
        "            done = int(50 * downloaded / total_length)\n",
        "            sys.stdout.write(\n",
        "                \"\\r{}[{}{}]\".format(file_path, \"█\" * done, \".\" * (50 - done))\n",
        "            )\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        try:\n",
        "            with open(file_path, \"wb\") as f:\n",
        "                self.client.download_fileobj(bucket, key, f, Callback=progress)\n",
        "            sys.stdout.write(\"\\n\")\n",
        "            sys.stdout.flush()\n",
        "        except:\n",
        "            raise Exception(f\"downloading file is failed. {url}\")\n",
        "        return file_path\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    s3 = AwsS3Downloader()\n",
        "\n",
        "    s3.download(\n",
        "        url=\"s3://skt-lsl-nlp-model/KoBERT/tokenizers/kobert_news_wiki_ko_cased-1087f8699e.spiece\",\n",
        "        local_dir=\".cache\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4ulWVfzB5OB"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiLW8gagCNXh"
      },
      "source": [
        "**kobert_utils**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ol4fTZY6COTx"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2019 SK T-Brain Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import hashlib\n",
        "import os\n",
        "\n",
        "\n",
        "def download(url, chksum=None, cachedir=\".cache\"):\n",
        "    cachedir_full = os.path.join(os.getcwd(), cachedir)\n",
        "    os.makedirs(cachedir_full, exist_ok=True)\n",
        "    filename = os.path.basename(url)\n",
        "    file_path = os.path.join(cachedir_full, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "        if hashlib.md5(open(file_path, \"rb\").read()).hexdigest()[:10] == chksum[:10]:\n",
        "            print(f\"using cached model. {file_path}\")\n",
        "            return file_path, True\n",
        "\n",
        "    s3 = AwsS3Downloader()\n",
        "    file_path = s3.download(url, cachedir_full)\n",
        "    if chksum:\n",
        "        assert (\n",
        "            chksum[:10] == hashlib.md5(open(file_path, \"rb\").read()).hexdigest()[:10]\n",
        "        ), \"corrupted file!\"\n",
        "    return file_path, False\n",
        "\n",
        "\n",
        "def get_tokenizer(cachedir=\".cache\"):\n",
        "    \"\"\"Get KoBERT Tokenizer file path after downloading\"\"\"\n",
        "    tokenizer = {\n",
        "        \"url\": \"s3://skt-lsl-nlp-model/KoBERT/tokenizers/kobert_news_wiki_ko_cased-1087f8699e.spiece\",\n",
        "        \"chksum\": \"ae5711deb3\",\n",
        "    }\n",
        "\n",
        "    model_info = tokenizer\n",
        "    model_path, is_cached = download(model_info[\"url\"], model_info[\"chksum\"], cachedir=cachedir)\n",
        "    return model_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1y1y2TmCNqh"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GUqen2eCmmU"
      },
      "source": [
        "**kobert_pytorch**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMFXwc-YI_l4",
        "outputId": "44cf9236-1dd5-4aff-b9d7-767b3502f193"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.24.3\n",
            "Uninstalling numpy-1.24.3:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/f2py3\n",
            "    /usr/local/bin/f2py3.10\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy-1.24.3.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libgfortran-040039e1.so.5.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled numpy-1.24.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy==1.23.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WR4mP6LJDLB",
        "outputId": "9b29e8b4-01db-41d8-c2b8-61ceabc9a271"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByLVxmL2Cmw_",
        "outputId": "14a26ecd-2d00-44d4-a284-c0930ed46713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/.cache/kobert_v1.zip\n",
            "using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
            "torch.Size([2, 768])\n",
            "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
            "tensor([[-0.2461,  0.2428,  0.2590,  ..., -0.4861, -0.0731,  0.0756],\n",
            "        [-0.2478,  0.2420,  0.2552,  ..., -0.4877, -0.0727,  0.0754],\n",
            "        [-0.2472,  0.2420,  0.2561,  ..., -0.4874, -0.0733,  0.0765]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2019 SK T-Brain Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "import torch\n",
        "from transformers import BertModel\n",
        "import gluonnlp\n",
        "\n",
        "#from kobert import download, get_tokenizer\n",
        "\n",
        "\n",
        "def get_pytorch_kobert_model(ctx=\"cpu\", cachedir=\".cache\"):\n",
        "    def get_kobert_model(model_path, vocab_file, ctx=\"cpu\"):\n",
        "        bertmodel = BertModel.from_pretrained(model_path, return_dict=False)\n",
        "        device = torch.device(ctx)\n",
        "        bertmodel.to(device)\n",
        "        bertmodel.eval()\n",
        "        vocab_b_obj = gluonnlp.vocab.BERTVocab.from_sentencepiece(\n",
        "            vocab_file, padding_token=\"[PAD]\"\n",
        "        )\n",
        "        return bertmodel, vocab_b_obj\n",
        "\n",
        "    pytorch_kobert = {\n",
        "        \"url\": \"s3://skt-lsl-nlp-model/KoBERT/models/kobert_v1.zip\",\n",
        "        \"chksum\": \"411b242919\",  # 411b2429199bc04558576acdcac6d498\n",
        "    }\n",
        "\n",
        "    # download model\n",
        "    model_info = pytorch_kobert\n",
        "    model_path, is_cached = download(\n",
        "        model_info[\"url\"], model_info[\"chksum\"], cachedir=cachedir\n",
        "    )\n",
        "    cachedir_full = os.path.expanduser(cachedir)\n",
        "    zipf = ZipFile(os.path.expanduser(model_path))\n",
        "    zipf.extractall(path=cachedir_full)\n",
        "    model_path = os.path.join(os.path.expanduser(cachedir), \"kobert_from_pretrained\")\n",
        "    # download vocab\n",
        "    vocab_path = get_tokenizer()\n",
        "    return get_kobert_model(model_path, vocab_path, ctx)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import torch\n",
        "    #from kobert import get_pytorch_kobert_model\n",
        "\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    model, vocab = get_pytorch_kobert_model()\n",
        "    sequence_output, pooled_output = model(input_ids, input_mask, token_type_ids)\n",
        "    print(pooled_output.shape)\n",
        "    print(vocab)\n",
        "    print(sequence_output[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_kjLVHuCm8a"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Eo-P9qW-O22"
      },
      "source": [
        "**setting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "R6mZo9jw-OH-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import logging\n",
        "import numpy as np\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "\n",
        "class Arguments():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.parser = ArgumentParser()\n",
        "\n",
        "    def add_type_of_processing(self):\n",
        "        self.add_argument('--opt_level', type=str, default='O1')\n",
        "        self.add_argument('--fp16', type=str, default='True')\n",
        "        self.add_argument('--train', type=str, default='True')\n",
        "        self.add_argument('--test', type=str, default='False')\n",
        "        #self.add_argument('--test', type=str, default='True')\n",
        "        self.add_argument('--device', type=str, default=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "\n",
        "    def add_hyper_parameters(self):\n",
        "        self.add_argument('--patient', type=int, default=10)\n",
        "        self.add_argument('--dropout', type=int, default=0.1)\n",
        "        self.add_argument('--max_len', type=int, default=50)\n",
        "        self.add_argument('--batch_size', type=int, default=256)\n",
        "        self.add_argument('--epochs', type=int, default=3)\n",
        "        self.add_argument('--eval_steps', type=int, default=250)\n",
        "        self.add_argument('--seed', type=int, default=1234)\n",
        "        self.add_argument('--lr', type=float, default=0.00005)\n",
        "        self.add_argument('--weight_decay', type=float, default=0.0)\n",
        "        self.add_argument('--warmup_ratio', type=float, default=0.05)\n",
        "        self.add_argument('--temperature', type=float, default=0.05)\n",
        "\n",
        "    def add_data_parameters(self):\n",
        "        self.add_argument('--train_data', type=str, default='snli_train_ko.tsv')\n",
        "        self.add_argument('--valid_data', type=str, default='sts-test.tsv')\n",
        "        self.add_argument('--test_data', type=str, default='xnli_test_ko.tsv')\n",
        "        self.add_argument('--task', type=str, default='content/drive/MyDrive/Colab Notebooks')\n",
        "        self.add_argument('--path_to_data', type=str, default='./data/')\n",
        "        #self.add_argument('--path_to_save', type=str, default='./output/')\n",
        "        self.add_argument('--path_to_save', type=str, default='/')\n",
        "        #self.add_argument('--path_to_saved_model', type=str, default='./output/')\n",
        "        self.add_argument('--path_to_saved_model', type=str, default='/')\n",
        "        self.add_argument('--ckpt', type=str, default='best_checkpoint.pt')\n",
        "\n",
        "    def print_args(self, args):\n",
        "        for idx, (key, value) in enumerate(args.__dict__.items()):\n",
        "            if idx == 0:print(\"argparse{\\n\", \"\\t\", key, \":\", value)\n",
        "            elif idx == len(args.__dict__) - 1:print(\"\\t\", key, \":\", value, \"\\n}\")\n",
        "            else:print(\"\\t\", key, \":\", value)\n",
        "\n",
        "    def add_argument(self, *args, **kw_args):\n",
        "        return self.parser.add_argument(*args, **kw_args)\n",
        "\n",
        "    def parse(self):\n",
        "        print(\"여기까지는 실행됨\")\n",
        "        args = self.parser.parse_args(\"\")\n",
        "        print(\"여기까지는 실행됨22222\")\n",
        "        self.print_args(args)\n",
        "        print(\"여기까지는 실행됨33333\")\n",
        "\n",
        "        return args\n",
        "\n",
        "\n",
        "class Setting():\n",
        "\n",
        "    def set_logger(self):\n",
        "\n",
        "        _logger = logging.getLogger()\n",
        "        formatter = logging.Formatter(\n",
        "            '[%(levelname)s] %(asctime)s [ %(message)s ] | file::%(filename)s | line::%(lineno)s')\n",
        "\n",
        "        stream_handler = logging.StreamHandler()\n",
        "        stream_handler.setFormatter(formatter)\n",
        "\n",
        "        _logger.addHandler(stream_handler)\n",
        "        _logger.setLevel(logging.DEBUG)\n",
        "\n",
        "        return _logger\n",
        "\n",
        "    def set_seed(self, args):\n",
        "\n",
        "        seed = args.seed\n",
        "\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        torch.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        parser = Arguments()\n",
        "        parser.add_type_of_processing()\n",
        "        parser.add_hyper_parameters()\n",
        "        parser.add_data_parameters()\n",
        "\n",
        "        args = parser.parse()\n",
        "        logger = self.set_logger()\n",
        "        self.set_seed(args)\n",
        "\n",
        "        return args, logger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkOQ43pS-Prt"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11OhgsqXAaFS"
      },
      "source": [
        "**dataloder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RkQdddHaAdLn"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy\n",
        "import torch\n",
        "import logging\n",
        "import gluonnlp as nlp\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "#from KoBERT.kobert.utils import get_tokenizer\n",
        "#from KoBERT.kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class ModelDataLoader(Dataset):\n",
        "    def __init__(self, file_path, args, metric, tokenizer, vocab, type):\n",
        "        self.type = type\n",
        "        self.args = args\n",
        "        self.vocab = vocab\n",
        "        self.metric = metric\n",
        "\n",
        "        \"\"\"NLI\"\"\"\n",
        "        self.anchor = []\n",
        "        self.positive = []\n",
        "        self.negative = []\n",
        "\n",
        "        \"\"\"STS\"\"\"\n",
        "        self.label = []\n",
        "        self.sentence_1 = []\n",
        "        self.sentence_2 = []\n",
        "\n",
        "        #  -------------------------------------\n",
        "        self.bert_tokenizer = tokenizer\n",
        "\n",
        "        self.transform = nlp.data.BERTSentenceTransform(\n",
        "            self.bert_tokenizer, max_seq_length=self.args.max_len, pad=True, pair=False)\n",
        "\n",
        "        self.file_path = file_path\n",
        "\n",
        "        \"\"\"\n",
        "        [CLS]: 2\n",
        "        [PAD]: 1\n",
        "        [UNK]: 0\n",
        "        \"\"\"\n",
        "        self.init_token = self.vocab.cls_token\n",
        "        self.pad_token = self.vocab.padding_token\n",
        "        self.unk_token = self.vocab.unknown_token\n",
        "\n",
        "        self.init_token_idx = self.vocab.token_to_idx[self.init_token]\n",
        "        self.pad_token_idx = self.vocab.token_to_idx[self.pad_token]\n",
        "        self.unk_token_idx = self.vocab.token_to_idx[self.unk_token]\n",
        "\n",
        "    def load_data(self, type):\n",
        "\n",
        "        with open(self.file_path) as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "            for line in lines:\n",
        "                self.data2tensor(line, type)\n",
        "\n",
        "        if type == 'train':\n",
        "            assert len(self.anchor) == len(self.positive) == len(self.negative)\n",
        "        else:\n",
        "            assert len(self.sentence_1) == len(self.sentence_2) == len(self.label)\n",
        "\n",
        "    def data2tensor(self, line, type):\n",
        "        split_data = line.split('\\t')\n",
        "\n",
        "        if type == 'train':\n",
        "            anchor, positive, negative = split_data\n",
        "            anchor = self.transform([anchor])\n",
        "            positive = self.transform([positive])\n",
        "            negative = self.transform([negative])\n",
        "\n",
        "            self.anchor.append(anchor)\n",
        "            self.positive.append(positive)\n",
        "            self.negative.append(negative)\n",
        "\n",
        "        else:\n",
        "            #print(split_data)\n",
        "            #sentence_1, sentence_2, label = split_data\n",
        "            genre, filename, year, id, label, sentence_1, sentence_2 = split_data\n",
        "            sentence_1 = self.transform([sentence_1])\n",
        "            sentence_2 = self.transform([sentence_2])\n",
        "\n",
        "            self.sentence_1.append(sentence_1)\n",
        "            self.sentence_2.append(sentence_2)\n",
        "            self.label.append(float(label.strip())/5.0)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        if self.type == 'train':\n",
        "            inputs = {'anchor': {\n",
        "                'source': torch.LongTensor(self.anchor[index][0]),\n",
        "                'valid_length': torch.tensor(self.anchor[index][1]),\n",
        "                'segment_ids': torch.LongTensor(self.anchor[index][2])\n",
        "                },\n",
        "                      'positive': {\n",
        "                'source': torch.LongTensor(self.positive[index][0]),\n",
        "                'valid_length': torch.tensor(self.positive[index][1]),\n",
        "                'segment_ids': torch.LongTensor(self.positive[index][2])\n",
        "                },\n",
        "                      'negative': {\n",
        "                'source': torch.LongTensor(self.negative[index][0]),\n",
        "                'valid_length': torch.tensor(self.negative[index][1]),\n",
        "                'segment_ids': torch.LongTensor(self.negative[index][2])\n",
        "                }}\n",
        "        else:\n",
        "\n",
        "            inputs = {'sentence_1': {\n",
        "                'source': torch.LongTensor(self.sentence_1[index][0]),\n",
        "                'valid_length': torch.tensor(self.sentence_1[index][1]),\n",
        "                'segment_ids': torch.LongTensor(self.sentence_1[index][2])\n",
        "                },\n",
        "                      'sentence_2': {\n",
        "                'source': torch.LongTensor(self.sentence_2[index][0]),\n",
        "                'valid_length': torch.tensor(self.sentence_2[index][1]),\n",
        "                'segment_ids': torch.LongTensor(self.sentence_2[index][2])\n",
        "                },\n",
        "                      'label': torch.FloatTensor([self.label[index]])}\n",
        "\n",
        "        inputs = self.metric.move2device(inputs, self.args.device)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.type == 'train':\n",
        "            return len(self.anchor)\n",
        "        else:\n",
        "            return len(self.label)\n",
        "\n",
        "\n",
        "# Get train, valid, test data loader and BERT tokenizer\n",
        "def get_loader(args, metric):\n",
        "    bert_model, vocab = get_pytorch_kobert_model()\n",
        "    tokenizer = get_tokenizer()\n",
        "    tokenizer = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "    path_to_train_data = '/' + args.task + '/' + args.train_data\n",
        "    # args.path_to_data + '/' + args.task + '/' + args.train_data\n",
        "    path_to_valid_data = '/' + args.task + '/' + args.valid_data\n",
        "    # args.path_to_data + '/' + args.task + '/' + args.valid_data\n",
        "    path_to_test_data = '/' + args.task + '/' + args.test_data\n",
        "    # args.path_to_data + '/' + args.task + '/' + args.test_data\n",
        "\n",
        "    if args.train == 'True' and args.test == 'False':\n",
        "        train_iter = ModelDataLoader(path_to_train_data, args, metric, tokenizer, vocab, type='train')\n",
        "        valid_iter = ModelDataLoader(path_to_valid_data, args, metric, tokenizer, vocab, type='valid')\n",
        "\n",
        "        train_iter.load_data('train')\n",
        "        valid_iter.load_data('valid')\n",
        "\n",
        "        loader = {'train': DataLoader(dataset=train_iter,\n",
        "                                      batch_size=args.batch_size,\n",
        "                                      shuffle=True),\n",
        "                  'valid': DataLoader(dataset=valid_iter,\n",
        "                                      batch_size=args.batch_size,\n",
        "                                      shuffle=True)}\n",
        "\n",
        "    elif args.train == 'False' and args.test == 'True':\n",
        "        test_iter = ModelDataLoader(path_to_test_data, args, metric, tokenizer, vocab, type='test')\n",
        "        test_iter.load_data('test')\n",
        "\n",
        "        loader = {'test': DataLoader(dataset=test_iter,\n",
        "                                     batch_size=args.batch_size,\n",
        "                                     shuffle=True)}\n",
        "\n",
        "    else:\n",
        "        loader = None\n",
        "\n",
        "    return bert_model, loader, tokenizer\n",
        "\n",
        "\n",
        "def convert_to_tensor(corpus, transform):\n",
        "    tensor_corpus = []\n",
        "    tensor_valid_length = []\n",
        "    tensor_segment_ids = []\n",
        "    for step, sentence in enumerate(corpus):\n",
        "        cur_sentence, valid_length, segment_ids = transform([sentence])\n",
        "\n",
        "        tensor_corpus.append(cur_sentence)\n",
        "        tensor_valid_length.append(numpy.array([valid_length]))\n",
        "        tensor_segment_ids.append(segment_ids)\n",
        "\n",
        "    inputs = {'source': torch.LongTensor(tensor_corpus),\n",
        "              'segment_ids': torch.LongTensor(tensor_segment_ids),\n",
        "              'valid_length': torch.tensor(tensor_valid_length)}\n",
        "\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def example_model_setting(model_ckpt):\n",
        "\n",
        "    from model.simcse.bert import BERT\n",
        "\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    bert_model, vocab = get_pytorch_kobert_model()\n",
        "    tokenizer = nlp.data.BERTSPTokenizer(get_tokenizer(), vocab, lower=False)\n",
        "    transform = nlp.data.BERTSentenceTransform(tokenizer, max_seq_length=50, pad=True, pair=False)\n",
        "\n",
        "    model = BERT(bert_model)\n",
        "\n",
        "    model.load_state_dict(torch.load(model_ckpt)['model'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return model, transform, device\n",
        "\n",
        "#gs = Setting()\n",
        "#args, logger = gs.run()\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#    get_loader('test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMq6pSIZAYIB"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om2uDsQa_PrV"
      },
      "source": [
        "**Processor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WAnt2C9h7K-",
        "outputId": "c0cb9144-68b0-4a75-fce8-4ab8305a5333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cxxfilt in /usr/local/lib/python3.10/dist-packages (0.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install cxxfilt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkBTv15biAu1",
        "outputId": "053fbdff-34c7-4051-91b4-7ad39df1fb89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnUagMYyiFmT",
        "outputId": "b90e5cf9-2e91-43d6-f59e-40567576c510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (6.0)\n"
          ]
        }
      ],
      "source": [
        "pip install PyYAML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2F_oWYDiRBB",
        "outputId": "cd495152-4b04-42d0-8bf7-dfc57326129c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (7.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.1.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "pip install pytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUBpMZfpiUkm",
        "outputId": "87116fb5-3ec5-49d3-9985-ad1bc843f145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.1)\n"
          ]
        }
      ],
      "source": [
        "pip install packaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOtpJUvdin6a",
        "outputId": "8f94c048-c64f-4835-9003-ea7d9c311d49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/apex\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "Warning: Torch did not find available GPUs on this system.\n",
            " If your intention is to cross-compile, this is not an error.\n",
            "By default, Apex will cross-compile for Pascal (compute capabilities 6.0, 6.1, 6.2),\n",
            "Volta (compute capability 7.0), Turing (compute capability 7.5),\n",
            "and, if the CUDA version is >= 11.0, Ampere (compute capability 8.0).\n",
            "If you wish to cross-compile for a single specific architecture,\n",
            "export TORCH_CUDA_ARCH_LIST=\"compute capability\" before running setup.py.\n",
            "\n",
            "\n",
            "\n",
            "torch.__version__  = 2.0.1+cu118\n",
            "\n",
            "\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing apex.egg-info/PKG-INFO\n",
            "writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "writing requirements to apex.egg-info/requires.txt\n",
            "writing top-level names to apex.egg-info/top_level.txt\n",
            "reading manifest file 'apex.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/apex\n",
            "copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/egg/apex\n",
            "copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/egg/apex\n",
            "creating build/bdist.linux-x86_64/egg/apex/mlp\n",
            "copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/egg/apex/mlp\n",
            "copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/egg/apex/mlp\n",
            "creating build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "creating build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "creating build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "creating build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib\n",
            "copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/fmha\n",
            "copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/fmha\n",
            "copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/egg/apex/contrib/fmha\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "copying build/lib/apex/contrib/transducer/_transducer_ref.py -> build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/clip_grad\n",
            "copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/egg/apex/contrib/clip_grad\n",
            "copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/clip_grad\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/focal_loss\n",
            "copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/egg/apex/contrib/focal_loss\n",
            "copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/focal_loss\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test\n",
            "copying build/lib/apex/contrib/test/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/fmha\n",
            "copying build/lib/apex/contrib/test/fmha/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/fmha\n",
            "copying build/lib/apex/contrib/test/fmha/test_fmha.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/fmha\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory\n",
            "copying build/lib/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory\n",
            "copying build/lib/apex/contrib/test/peer_memory/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/transducer\n",
            "copying build/lib/apex/contrib/test/transducer/test_transducer_joint.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/transducer\n",
            "copying build/lib/apex/contrib/test/transducer/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/transducer\n",
            "copying build/lib/apex/contrib/test/transducer/test_transducer_loss.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/transducer\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad\n",
            "copying build/lib/apex/contrib/test/clip_grad/test_clip_grad.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad\n",
            "copying build/lib/apex/contrib/test/clip_grad/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu\n",
            "copying build/lib/apex/contrib/test/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu\n",
            "copying build/lib/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck\n",
            "copying build/lib/apex/contrib/test/bottleneck/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck\n",
            "copying build/lib/apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers\n",
            "copying build/lib/apex/contrib/test/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers\n",
            "copying build/lib/apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers\n",
            "copying build/lib/apex/contrib/test/optimizers/test_dist_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm\n",
            "copying build/lib/apex/contrib/test/layer_norm/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm\n",
            "copying build/lib/apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy\n",
            "copying build/lib/apex/contrib/test/xentropy/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy\n",
            "copying build/lib/apex/contrib/test/xentropy/test_label_smoothing.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss\n",
            "copying build/lib/apex/contrib/test/focal_loss/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss\n",
            "copying build/lib/apex/contrib/test/focal_loss/test_focal_loss.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn\n",
            "copying build/lib/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn\n",
            "copying build/lib/apex/contrib/test/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d\n",
            "copying build/lib/apex/contrib/test/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d\n",
            "copying build/lib/apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn\n",
            "copying build/lib/apex/contrib/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn\n",
            "copying build/lib/apex/contrib/cudnn_gbn/batch_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu\n",
            "copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu\n",
            "copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n",
            "copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n",
            "copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d\n",
            "copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d\n",
            "copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d\n",
            "creating build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "creating build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib/apex/transformer/_ucc_util.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/functional\n",
            "copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/egg/apex/transformer/functional\n",
            "copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/functional\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/_data\n",
            "copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/egg/apex/transformer/_data\n",
            "copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/_data\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/amp\n",
            "copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/amp\n",
            "copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/egg/apex/transformer/amp\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/layers\n",
            "copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/egg/apex/transformer/layers\n",
            "copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/layers\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "creating build/bdist.linux-x86_64/egg/apex/fused_dense\n",
            "copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/egg/apex/fused_dense\n",
            "copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/egg/apex/fused_dense\n",
            "creating build/bdist.linux-x86_64/egg/apex/normalization\n",
            "copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/egg/apex/normalization\n",
            "copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/egg/apex/normalization\n",
            "creating build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/_autocast_utils.py to _autocast_utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/mlp/mlp.py to mlp.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/mlp/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/RNNBackend.py to RNNBackend.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/models.py to models.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/cells.py to cells.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/opt.py to opt.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/wrap.py to wrap.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/rnn_compat.py to rnn_compat.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/frontend.py to frontend.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_amp_state.py to _amp_state.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_initialize.py to _initialize.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/scaler.py to scaler.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_process_optimizer.py to _process_optimizer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/handle.py to handle.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/amp.py to amp.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/__version__.py to __version__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/compat.py to compat.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_novograd.py to fused_novograd.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_lamb.py to fused_lamb.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_mixed_precision_lamb.py to fused_mixed_precision_lamb.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_adam.py to fused_adam.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_sgd.py to fused_sgd.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/fmha/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/fmha/fmha.py to fmha.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/peer_memory/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/peer_memory/peer_memory.py to peer_memory.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/peer_memory/peer_halo_exchanger_1d.py to peer_halo_exchanger_1d.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/transducer/_transducer_ref.py to _transducer_ref.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/transducer/transducer.py to transducer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/transducer/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/clip_grad/clip_grad.py to clip_grad.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/clip_grad/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/focal_loss/focal_loss.py to focal_loss.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/focal_loss/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/fmha/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/fmha/test_fmha.py to test_fmha.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py to test_peer_halo_exchange_module.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/transducer/test_transducer_joint.py to test_transducer_joint.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/transducer/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/transducer/test_transducer_loss.py to test_transducer_loss.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad/test_clip_grad.py to test_clip_grad.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py to test_encdec_multihead_attn_norm_add.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py to test_mha_fused_softmax.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py to test_encdec_multihead_attn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_self_multihead_attn.py to test_self_multihead_attn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py to test_self_multihead_attn_norm_add.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py to test_fast_self_multihead_attn_bias.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py to test_conv_bias_relu.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck/test_bottleneck_module.py to test_bottleneck_module.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers/test_distributed_fused_lamb.py to test_distributed_fused_lamb.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers/test_dist_adam.py to test_dist_adam.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm/test_fast_layer_norm.py to test_fast_layer_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy/test_label_smoothing.py to test_label_smoothing.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss/test_focal_loss.py to test_focal_loss.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py to test_cudnn_gbn_with_two_gpus.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d/test_index_mul_2d.py to test_index_mul_2d.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn/batch_norm.py to batch_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/groupbn/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/asp.py to asp.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_lib.py to permutation_lib.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py to permutation_utilities.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py to call_permutation_search_kernels.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py to exhaustive_search.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py to channel_swap.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu/conv_bias_relu.py to conv_bias_relu.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/test.py to test.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/halo_exchangers.py to halo_exchangers.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/bottleneck.py to bottleneck.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/layer_norm/layer_norm.py to layer_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/layer_norm/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/xentropy/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d/index_mul_2d.py to index_mul_2d.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/multi_tensor_apply/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/fp16util.py to fp16util.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/log_util.py to log_util.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/microbatches.py to microbatches.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/enums.py to enums.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/parallel_state.py to parallel_state.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/_ucc_util.py to _ucc_util.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/functional/fused_softmax.py to fused_softmax.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/functional/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/_data/_batchsampler.py to _batchsampler.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/_data/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/amp/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/amp/grad_scaler.py to grad_scaler.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/commons.py to commons.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/standalone_transformer_lm.py to standalone_transformer_lm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/distributed_test_base.py to distributed_test_base.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/arguments.py to arguments.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/global_vars.py to global_vars.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/standalone_gpt.py to standalone_gpt.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/standalone_bert.py to standalone_bert.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/p2p_communication.py to p2p_communication.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/_timers.py to _timers.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py to fwd_bwd_pipelining_with_interleaving.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py to fwd_bwd_pipelining_without_interleaving.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/common.py to common.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py to fwd_bwd_no_pipelining.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/layers/layer_norm.py to layer_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/layers/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/mappings.py to mappings.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/data.py to data.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/cross_entropy.py to cross_entropy.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/memory.py to memory.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/layers.py to layers.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/random.py to random.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fused_dense/fused_dense.py to fused_dense.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fused_dense/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/LARC.py to LARC.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/multiproc.py to multiproc.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/distributed.py to distributed.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/__init__.py to __init__.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/apex-0.1-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing apex-0.1-py3.10.egg\n",
            "Removing /usr/local/lib/python3.10/dist-packages/apex-0.1-py3.10.egg\n",
            "Copying apex-0.1-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "apex 0.1 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/apex-0.1-py3.10.egg\n",
            "Processing dependencies for apex==0.1\n",
            "Searching for packaging==23.1\n",
            "Best match: packaging 23.1\n",
            "Adding packaging 23.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Finished processing dependencies for apex==0.1\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/apex\n",
        "!python3 setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LOUxm7h1_O7M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from apex import amp\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import torch.quantization\n",
        "import torch.optim as optim\n",
        "#from data.dataloader import get_loader\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class Processor():\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.config = None\n",
        "        self.metric = Metric(args)\n",
        "        self.loss = Loss(args)\n",
        "        self.total_steps = 0\n",
        "        self.model_checker = {'early_stop': False,\n",
        "                              'early_stop_patient': 0,\n",
        "                              'best_valid_score': 0}\n",
        "        self.dev_progress = {'score': 0, 'iter': 0}\n",
        "        self.model_progress = {'loss': 0, 'iter': 0}\n",
        "\n",
        "    def run(self, inputs, indicator=None, type=None):\n",
        "\n",
        "        if type == 'train':\n",
        "            anchor_embeddings, positive_embeddings, negative_embeddings = self.config['model'](inputs, type)\n",
        "            loss = self.loss.train_loss_fct(self.config, anchor_embeddings, positive_embeddings, negative_embeddings)\n",
        "            return loss\n",
        "        else:\n",
        "            sentence_1_embeddings, sentence_2_embeddings = self.config['model'](inputs, type)\n",
        "            score = self.loss.evaluation_during_training(sentence_1_embeddings, sentence_2_embeddings, inputs['label'], indicator)\n",
        "            return score\n",
        "\n",
        "    def progress(self, loss):\n",
        "        self.model_progress['loss'] += loss\n",
        "        self.model_progress['iter'] += 1\n",
        "\n",
        "    def progress_validation(self, score):\n",
        "        self.dev_progress['score'] += score\n",
        "        self.dev_progress['iter'] += 1\n",
        "\n",
        "    def return_value(self):\n",
        "        loss = self.model_progress['loss'].data.cpu().numpy() / self.model_progress['iter']\n",
        "        acc = self.model_progress['acc'].data.cpu().numpy() / self.model_progress['iter']\n",
        "\n",
        "        return loss, acc\n",
        "\n",
        "    def get_object(self, tokenizer, model):\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay': self.args.weight_decay},\n",
        "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(optimizer_grouped_parameters, lr=self.args.lr)\n",
        "        print(\"좌표 : get_object 실행 완료\")\n",
        "        return criterion, optimizer\n",
        "\n",
        "    def get_scheduler(self, optim, train_loader):\n",
        "        print(\"좌표 : get_scheduler 진입\")\n",
        "        print(optim)\n",
        "        print(train_loader)\n",
        "        train_total = len(train_loader) * self.args.epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(optim,\n",
        "                                                    num_warmup_steps=self.args.warmup_ratio * train_total,\n",
        "                                                    num_training_steps=train_total)\n",
        "\n",
        "        return scheduler, train_total\n",
        "\n",
        "    def model_setting(self):\n",
        "        model, loader, tokenizer = get_loader(self.args, self.metric)\n",
        "        model = BERT(model)\n",
        "        model.to(self.args.device)\n",
        "\n",
        "        criterion, optimizer = self.get_object(tokenizer, model)\n",
        "\n",
        "        print(\"좌표 : model_setting에서 if문 들어가기 직전\")\n",
        "        if self.args.train == 'True':\n",
        "            print(\"좌표 : model_setting에서 self.args.train이 참인경우\")\n",
        "            print(\"좌표 : get_scheduler함수 안에 들어가기 전 optimizer\", optimizer)\n",
        "            print(\"좌표 : get_scheduler함수 안에 들어가기 전 loader['train']\", loader['train'])\n",
        "            scheduler, total_steps = self.get_scheduler(optimizer, loader['train'])\n",
        "            self.total_steps = total_steps\n",
        "        else:\n",
        "            scheduler = None\n",
        "\n",
        "        config = {'loader': loader,\n",
        "                  'optimizer': optimizer,\n",
        "                  'criterion': criterion,\n",
        "                  'scheduler': scheduler,\n",
        "                  'tokenizer': tokenizer,\n",
        "                  'args': self.args,\n",
        "                  'model': model}\n",
        "\n",
        "        if config['args'].fp16 == 'True':\n",
        "            config['model'], config['optimizer'] = amp.initialize(\n",
        "                config['model'], config['optimizer'], opt_level=config['args'].opt_level)\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        return self.config\n",
        "\n",
        "    def train(self, epoch):\n",
        "        self.config['model'].train()\n",
        "\n",
        "        for step, batch in enumerate(tqdm(self.config['loader']['train'])):\n",
        "            self.config['optimizer'].zero_grad()\n",
        "\n",
        "            inputs = batch\n",
        "\n",
        "            train_loss = self.run(inputs, type='train')\n",
        "\n",
        "            if self.args.fp16 == 'True':\n",
        "                with amp.scale_loss(train_loss, self.config['optimizer']) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                train_loss.backward()\n",
        "\n",
        "            self.config['optimizer'].step()\n",
        "            self.config['scheduler'].step()\n",
        "\n",
        "            self.progress(train_loss.data)\n",
        "\n",
        "            if self.model_progress['iter'] % self.args.eval_steps == 0 or self.model_progress['iter'] == self.total_steps:\n",
        "                valid_score = self.valid()\n",
        "                performance = {'tl': train_loss, 'vs': valid_score, 'ep': epoch, 'step': self.model_progress['iter']}\n",
        "                \n",
        "                self.metric.save_model(self.config, performance, self.model_checker)\n",
        "                self.config['model'].train()\n",
        "                \n",
        "    def valid(self):\n",
        "        self.config['model'].eval()\n",
        "        self.dev_progress = self.dev_progress.fromkeys(self.dev_progress, 0)\n",
        "\n",
        "        score_indicator = {'eval_pearson_cosine': 0,\n",
        "                           'eval_spearman_cosine': 0,\n",
        "                           'eval_pearson_manhattan': 0,\n",
        "                           'eval_spearman_manhattan': 0,\n",
        "                           'eval_pearson_euclidean': 0,\n",
        "                           'eval_spearman_euclidean': 0,\n",
        "                           'eval_pearson_dot': 0,\n",
        "                           'eval_spearman_dot': 0}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for step, batch in enumerate(self.config['loader']['valid']):\n",
        "                inputs = batch\n",
        "                score = self.run(inputs, indicator=score_indicator, type='valid')\n",
        "\n",
        "                self.progress_validation(score)\n",
        "\n",
        "        score = self.metric.cal_dev_score(self.dev_progress, score_indicator)\n",
        "\n",
        "        return score\n",
        "\n",
        "    def test(self):\n",
        "        self.config['model'].load_state_dict(torch.load(self.args.path_to_saved_model)['model'], strict=False)\n",
        "        self.config['model'].eval()\n",
        "        self.dev_progress = self.dev_progress.fromkeys(self.dev_progress, 0)\n",
        "\n",
        "        score_indicator = {'eval_pearson_cosine': 0,\n",
        "                           'eval_spearman_cosine': 0,\n",
        "                           'eval_pearson_manhattan': 0,\n",
        "                           'eval_spearman_manhattan': 0,\n",
        "                           'eval_pearson_euclidean': 0,\n",
        "                           'eval_spearman_euclidean': 0,\n",
        "                           'eval_pearson_dot': 0,\n",
        "                           'eval_spearman_dot': 0}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for step, batch in enumerate(self.config['loader']['test']):\n",
        "                inputs = batch\n",
        "                score = self.run(inputs, indicator=score_indicator, type='test')\n",
        "\n",
        "                self.progress_validation(score)\n",
        "\n",
        "        logger.info('### TEST SCORE ###')\n",
        "        score = self.metric.cal_dev_score(self.dev_progress, score_indicator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1Gg16QN_BFT"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hdcWuM7-IkE"
      },
      "source": [
        "**main**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HAPifM4E-Cqo",
        "outputId": "17329574-de90-463a-9eb5-7fb3b0161ad0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "여기까지는 실행됨\n",
            "여기까지는 실행됨22222\n",
            "argparse{\n",
            " \t opt_level : O1\n",
            "\t fp16 : True\n",
            "\t train : True\n",
            "\t test : False\n",
            "\t device : cpu\n",
            "\t patient : 10\n",
            "\t dropout : 0.1\n",
            "\t max_len : 50\n",
            "\t batch_size : 256\n",
            "\t epochs : 3\n",
            "\t eval_steps : 250\n",
            "\t seed : 1234\n",
            "\t lr : 5e-05\n",
            "\t weight_decay : 0.0\n",
            "\t warmup_ratio : 0.05\n",
            "\t temperature : 0.05\n",
            "\t train_data : snli_train_ko.tsv\n",
            "\t valid_data : sts-test.tsv\n",
            "\t test_data : xnli_test_ko.tsv\n",
            "\t task : content\n",
            "\t path_to_data : ./data/\n",
            "\t path_to_save : /\n",
            "\t path_to_saved_model : /\n",
            "\t ckpt : best_checkpoint.pt \n",
            "}\n",
            "여기까지는 실행됨33333\n",
            "using cached model. /content/drive/MyDrive/Colab Notebooks/apex/.cache/kobert_v1.zip\n",
            "using cached model. /content/drive/MyDrive/Colab Notebooks/apex/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
            "using cached model. /content/drive/MyDrive/Colab Notebooks/apex/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
            "좌표 : get_object 실행 완료\n",
            "좌표 : model_setting에서 if문 들어가기 직전\n",
            "좌표 : model_setting에서 self.args.train이 참인경우\n",
            "좌표 : get_scheduler함수 안에 들어가기 전 optimizer AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 5e-05\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 5e-05\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            ")\n",
            "좌표 : get_scheduler함수 안에 들어가기 전 loader['train'] <torch.utils.data.dataloader.DataLoader object at 0x7faf8609f8e0>\n",
            "좌표 : get_scheduler 진입\n",
            "AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 5e-05\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 5e-05\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            ")\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7faf8609f8e0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/apex/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-b0f6c989bc6a>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSetting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-b0f6c989bc6a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args, logger)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_setting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Setting Complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-e74605ce6e4b>\u001b[0m in \u001b[0;36mmodel_setting\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'args'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'True'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             config['model'], config['optimizer'] = amp.initialize(\n\u001b[0m\u001b[1;32m    106\u001b[0m                 config['model'], config['optimizer'], opt_level=config['args'].opt_level)\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/apex/apex/amp/frontend.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(models, optimizers, enabled, opt_level, cast_model_type, patch_torch_functions, keep_batchnorm_fp32, master_weights, loss_scale, cast_model_outputs, num_losses, verbosity, min_loss_scale, max_loss_scale)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mmaybe_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{:22} : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_amp_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast_model_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/apex/apex/amp/_initialize.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(models, optimizers, properties, num_losses, cast_model_outputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_amp_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_incoming_model_not_fp32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mcheck_params_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# In the future, when FP16_Optimizer can be deprecated and master weights can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/apex/apex/amp/_initialize.py\u001b[0m in \u001b[0;36mcheck_params_fp32\u001b[0;34m(models)\u001b[0m\n\u001b[1;32m     89\u001b[0m                         name, param.type()))\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     warn_or_err(\"Found param {} with type {}, expected torch.cuda.FloatTensor.\\n\"\n\u001b[0m\u001b[1;32m     92\u001b[0m                         \u001b[0;34m\"When using amp.initialize, you need to provide a model with parameters\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                         \u001b[0;34m\"located on a CUDA device before passing it no matter what optimization level\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/apex/apex/amp/_amp_state.py\u001b[0m in \u001b[0;36mwarn_or_err\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Warning:  \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# I'm not sure if allowing hard_override is a good idea.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# + \"  If you're sure you know what you're doing, supply \" +\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found param bert.embeddings.word_embeddings.weight with type torch.FloatTensor, expected torch.cuda.FloatTensor.\nWhen using amp.initialize, you need to provide a model with parameters\nlocated on a CUDA device before passing it no matter what optimization level\nyou chose. Use model.to('cuda') to use the default device."
          ]
        }
      ],
      "source": [
        "#from model.setting import Setting, Arguments\n",
        "#from model.simcse.processor import Processor\n",
        "\n",
        "\n",
        "def main(args, logger) -> None:\n",
        "    processor = Processor(args)\n",
        "    config = processor.model_setting()\n",
        "    logger.info('Model Setting Complete')\n",
        "\n",
        "    if args.train == 'True':\n",
        "        logger.info('Start Training')\n",
        "        \n",
        "        for epoch in range(args.epochs):\n",
        "            processor.train(epoch+1)\n",
        "\n",
        "    if args.test == 'True':\n",
        "        logger.info(\"Start Test\")\n",
        "        \n",
        "        processor.test()\n",
        "        processor.metric.print_size_of_model(config['model'])\n",
        "        processor.metric.count_parameters(config['model'])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args, logger = Setting().run()\n",
        "    main(args, logger)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}